{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT-LR-TC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1mr4nEMZ9GrKQxROcKKEqT0sSuZw29vzK",
      "authorship_tag": "ABX9TyNyOFgRwpySxy2AGuYlXTKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someshsingh22/News-Propaganda-Detection/blob/master/BERT_LR_TC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKWbGL-P4u24",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/someshsingh22/News-Propaganda-Detection\n",
        "!pip install transformers\n",
        "%cd News-Propaganda-Detection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMRRCuFVEgRm",
        "colab_type": "code",
        "outputId": "af4fd600-77ea-4701-f05a-3523be7e30ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from transformers import *\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import itertools\n",
        "from tqdm import tqdm\n",
        "from tqdm import  tqdm_notebook\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder as LE\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch\n",
        "import glob\n",
        "import os.path\n",
        "import sys\n",
        "import codecs\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import random\n",
        "torch.manual_seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11bZhBv0ItG9",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "def read_articles_from_file_list(folder_name, file_pattern=\"*.txt\"):\n",
        "    \"\"\"\n",
        "    Read articles from files matching patterns <file_pattern> from  \n",
        "    the directory <folder_name>. \n",
        "    The content of the article is saved in the dictionary whose key\n",
        "    is the id of the article (extracted from the file name).\n",
        "    Each element of <sentence_list> is one line of the article.\n",
        "    \"\"\"\n",
        "    file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
        "    articles = {}\n",
        "    article_id_list, sentence_id_list, sentence_list = ([], [], [])\n",
        "    for filename in sorted(file_list):\n",
        "        article_id = os.path.basename(filename).split(\".\")[0][7:]\n",
        "        with codecs.open(filename, \"r\", encoding=\"utf8\") as f:\n",
        "            articles[article_id] = f.read()\n",
        "    return articles\n",
        "\n",
        "\n",
        "def read_predictions_from_file(filename):\n",
        "    \"\"\"\n",
        "    Reader for the gold file and the template output file. \n",
        "    Return values are four arrays with article ids, labels \n",
        "    (or ? in the case of a template file), begin of a fragment, \n",
        "    end of a fragment. \n",
        "    \"\"\"\n",
        "    articles_id, span_starts, span_ends, gold_labels = ([], [], [], [])\n",
        "    with open(filename, \"r\") as f:\n",
        "        for row in f.readlines():\n",
        "            article_id, gold_label, span_start, span_end = row.rstrip().split(\"\\t\")\n",
        "            articles_id.append(article_id)\n",
        "            gold_labels.append(gold_label)\n",
        "            span_starts.append(span_start)\n",
        "            span_ends.append(span_end)\n",
        "    return articles_id, span_starts, span_ends, gold_labels\n",
        "\n",
        "def report(true, pred):\n",
        "    cm=confusion_matrix(true, pred)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.figure(figsize = (10,8))\n",
        "    sns.heatmap(cm,annot=True)\n",
        "    cf_rep=classification_report(true,pred)\n",
        "    print(cf_rep)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db6ZFmewFQPd",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "tfidf_c_config = {\"min\": 5, \"ng_l\": 1, \"ng_h\": 6, \"max_features\": 1500}\n",
        "\n",
        "tfidf_w_config = {\"min\": 3, \"ng_l\": 1, \"ng_h\": 3, \"max_features\": 2000}\n",
        "\n",
        "\n",
        "class FeatureExtraction:\n",
        "    def __init__(\n",
        "        self,\n",
        "        train_data,\n",
        "        dev_data,\n",
        "        tfidf_c_config=tfidf_c_config,\n",
        "        tfidf_w_config=tfidf_w_config,\n",
        "    ):\n",
        "        self.train_data, self.dev_data = train_data, dev_data\n",
        "        self.tfidf_c = TfidfVectorizer(\n",
        "            sublinear_tf=True,\n",
        "            min_df=tfidf_c_config[\"min\"],\n",
        "            ngram_range=(tfidf_c_config[\"ng_l\"], tfidf_c_config[\"ng_h\"]),\n",
        "            stop_words=\"english\",\n",
        "            analyzer=\"char\",\n",
        "            max_features=tfidf_c_config[\"max_features\"],\n",
        "            lowercase=train_data.lower,\n",
        "        )\n",
        "        self.tfidf_w = TfidfVectorizer(\n",
        "            sublinear_tf=True,\n",
        "            min_df=tfidf_w_config[\"min\"],\n",
        "            ngram_range=(tfidf_w_config[\"ng_l\"], tfidf_w_config[\"ng_h\"]),\n",
        "            stop_words=\"english\",\n",
        "            analyzer=\"word\",\n",
        "            max_features=tfidf_w_config[\"max_features\"],\n",
        "            lowercase=dev_data.lower,\n",
        "        )\n",
        "\n",
        "    def get_features(self):\n",
        "        sentences = self.train_data.sentences + self.dev_data.sentences\n",
        "        spans = np.asarray(self.train_data.spans + self.dev_data.spans).reshape(-1, 1)\n",
        "        sentences_c = self.tfidf_c.fit_transform(sentences)\n",
        "        sentences_w = self.tfidf_w.fit_transform(sentences)\n",
        "        sen_tc, sen_dc = (\n",
        "            sentences_c[: self.train_data.size],\n",
        "            sentences_c[self.train_data.size :],\n",
        "        )\n",
        "        sen_tw, sen_dw = (\n",
        "            sentences_w[: self.train_data.size],\n",
        "            sentences_w[self.train_data.size :],\n",
        "        )\n",
        "        span_t, span_d = spans[: self.train_data.size], spans[self.train_data.size :]\n",
        "        self.train_features = hstack([sen_tc, sen_tw, span_t])\n",
        "        self.dev_features = hstack([sen_dc, sen_dw, span_d])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRaQlHytFYi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel:\n",
        "  def __init__(self, device=None, transformer=None, seed=1234):\n",
        "    self.device=device\n",
        "    self.train_loss_set = []\n",
        "    self.predictions=[]\n",
        "\n",
        "    if device is None:\n",
        "      self.device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    if transformer is None:\n",
        "      self.transformer=BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=14).to(self.device)\n",
        "    else:\n",
        "      self.transformer=transformer.to(self.device)\n",
        "      \n",
        "    self.__seed=seed\n",
        "    self.seed()\n",
        "\n",
        "  def seed(self):\n",
        "    np.random.seed(self.__seed)\n",
        "    random.seed(self.__seed)\n",
        "    torch.manual_seed(self.__seed)\n",
        "    if self.device == 'cuda':\n",
        "      torch.cuda.manual_seed(self.__seed)\n",
        "      torch.cuda.manual_seed_all(self.__seed)\n",
        "      torch.backends.cudnn.enabled = False \n",
        "      torch.backends.cudnn.benchmark = False\n",
        "      torch.backends.cudnn.deterministic = True\n",
        "\n",
        "  def freeze(self, condition=None):\n",
        "    if condition is None:\n",
        "      condition = lambda name : True if 'classifier' in name or 'pooler' in name or '11' in name or '10' in name else False\n",
        "    for name, param in self.transformer.named_parameters():\n",
        "      param.requires_grad=condition(name)\n",
        "\n",
        "  def updater(self,optimizer=None, lr=1e-4, scheduler=None):\n",
        "    self.optimizer =optimizer\n",
        "    if self.optimizer is None:\n",
        "      self.optimizer = AdamW(self.transformer.parameters(), lr=lr, correct_bias=False)\n",
        "    if scheduler is None:\n",
        "      max_grad_norm = 1.0\n",
        "      num_training_steps = 1000\n",
        "      num_warmup_steps = 100\n",
        "      self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "      self.clip=max_grad_norm\n",
        "    elif scheduler != False:\n",
        "      self.scheduler=scheduler\n",
        "      \n",
        "\n",
        "  def train(self,train_dataset, valid_dataset, epochs=1,verbosity=4):\n",
        "    total_step = len(train_dataset.dataloader)\n",
        "    verbosity=total_step/verbosity\n",
        "\n",
        "    for epoch in tqdm_notebook(range(epochs)):\n",
        "        tr_loss = 0\n",
        "        nb_tr_examples, nb_tr_steps = 0, 0\n",
        "        for i, batch in enumerate(train_dataset.dataloader):\n",
        "          batch = tuple(t.to(self.device) for t in batch)\n",
        "          b_input_ids, b_input_mask, b_labels = batch\n",
        "          outputs = self.transformer(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "          loss = outputs[0]\n",
        "          tr_loss+=loss.item() \n",
        "          loss.backward()\n",
        "          if self.scheduler != False:\n",
        "          \ttorch.nn.utils.clip_grad_norm_(self.transformer.parameters(), self.clip)\n",
        "          self.optimizer.step()\n",
        "          if self.scheduler != False:\n",
        "          \tself.scheduler.step()\n",
        "          self.optimizer.zero_grad()\n",
        "          \n",
        "          #if i % verbosity == verbosity-1:\n",
        "            #print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, total_step, tr_loss/i))\n",
        "\n",
        "        train_epoch_accuracy = self.evaluate(train_dataset, mode='train')\n",
        "        valid_epoch_accuracy = self.evaluate(valid_dataset, mode='valid')\n",
        "        print ('\\033[1m'+'Epoch [{}/{}], Train_micro_avg: {:.4f}, Valid_micro_avg: {:.4f}'.format(epoch+1, epochs,train_epoch_accuracy, valid_epoch_accuracy)+'\\033[0m')\n",
        "\n",
        "  def evaluate(self, dataset, mode = 'train'):\n",
        "    with torch.no_grad():\n",
        "      correct, total = 0, 0\n",
        "      true=[]\n",
        "      for i, batch in enumerate(dataset.dataloader):\n",
        "        batch = tuple(t.to(self.device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs = self.transformer(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        prediction = torch.argmax(outputs[0],dim=1)\n",
        "        total += b_labels.size(0)\n",
        "        true += [b_labels.cpu()]\n",
        "        correct+=(prediction==b_labels).sum().item()\n",
        "        if mode == 'test':\n",
        "          self.predictions.extend(list(np.asarray(prediction.cpu())))\n",
        "      \n",
        "      if mode == 'train' or mode == 'valid':\n",
        "        return (100*correct/total)\n",
        "      else:\n",
        "        self.predictions = dataset.le.inverse_transform(self.predictions)\n",
        "        return None\n",
        "  \n",
        "  def predict(self, test_dataset):\n",
        "    self.evaluate(test_dataset, mode='test')\n",
        "    return self.predictions\n",
        "\n",
        "  def logits(self, dataset):\n",
        "    logits=[]\n",
        "    with torch.no_grad():\n",
        "      for i, batch in enumerate(dataset.dataloader):\n",
        "        batch = tuple(t.to(self.device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        outputs = self.transformer(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        logits.extend(list(np.asarray(outputs[0].cpu())))\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lro0L1gUGovD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, articles_folder, labels_file):\n",
        "        self.articles_folder = articles_folder\n",
        "        self.labels_file = labels_file\n",
        "        self.articles = read_articles_from_file_list(articles_folder)\n",
        "        self.read()\n",
        "\n",
        "    def read(self):\n",
        "    \tarticles_id, span_starts, span_ends, self.gold_labels = read_predictions_from_file(self.labels_file)\n",
        "    \tself.spans = [int(end)-int(start) for start, end in zip(span_starts, span_ends)]\n",
        "    \tprint(\"Read %d annotations from %d articles\" % (len(span_starts), len(set(articles_id))))\n",
        "    \tself.sentences=[self.articles[id][int(start):int(end)] for id, start, end in zip(articles_id, span_starts, span_ends)]\n",
        "    \tself.size=len(self.sentences)\n",
        "\n",
        "\n",
        "\n",
        "class SLDataset(Dataset):\n",
        "    def __init__(self,  articles_folder=None, labels_file=None, lower=True):\n",
        "        super().__init__(articles_folder, labels_file)\n",
        "        self.lower=lower\n",
        "\n",
        "    def clean(self):\n",
        "        def text_clean(text):\n",
        "            if self.lower:\n",
        "                text=text.lower()\n",
        "            text=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "            text=re.sub('[“\"”]',' \" ',text)\n",
        "            if self.lower:\n",
        "                retain='[^abcdefghijklmnopqrstuvwxyz!#?\". ]'\n",
        "            else:\n",
        "                retain='[^abcdefghijklmnopqrstuvwxyzQWERTYUIOPASDFGHJKLZXCVBNM!#?\". ]'\n",
        "            text=re.sub('[()–-]',' ',text)\n",
        "            text=re.sub(retain,'',text)\n",
        "            text=re.sub('[.]',' . ',text)\n",
        "            text=text.replace('?',' ? ')\n",
        "            text=text.replace('#',' # ')\n",
        "            text=text.replace('!',' ! ')\n",
        "            return ' '.join(text.split())\n",
        "        \n",
        "        print(\"Cleaning Sentences\")\n",
        "        self.sentences=[text_clean(sentence) for sentence in self.sentences]\n",
        "\n",
        "class TransformerDataset(Dataset):\n",
        "    def __init__(self, articles_folder=None, labels_file=None):\n",
        "        super().__init__(articles_folder, labels_file)\n",
        "        self.clean()\n",
        "        self.sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in self.sentences]\n",
        "        self.le=LE()\n",
        "        self.labels=self.le.fit_transform(self.gold_labels)\n",
        "\n",
        "    def clean(self):\n",
        "        def text_clean(text):\n",
        "            text=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "            text=re.sub('[“\"”]',' \" ',text)\n",
        "            retain='[^abcdefghijklmnopqrstuvwxyzQWERTYUIOPASDFGHJKLZXCVBNM!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~.0123456789 ]'\n",
        "            return ' '.join(text.split())\n",
        "        \n",
        "        print(\"Cleaning Sentences\")\n",
        "        self.sentences=[text_clean(sentence) for sentence in self.sentences]\n",
        "        \n",
        "    def tokenize(self, tokenizer=BertTokenizer.from_pretrained('bert-base-uncased'), verbosity=True):\n",
        "        self.tokenizer=tokenizer\n",
        "        print(\"Tokenizing\")\n",
        "        self.tokenized_texts = [self.tokenizer.tokenize(sent) for sent in self.sentences]\n",
        "        if verbosity:\n",
        "          print(\"Tokenized \\n\", self.tokenized_texts[0])\n",
        "    \n",
        "    def encode(self, MAX_LEN=90):\n",
        "      input_ids=[]\n",
        "      for i in tqdm_notebook(range(len(self.tokenized_texts))):\n",
        "        input_ids.append(self.tokenizer.convert_tokens_to_ids(self.tokenized_texts[i]))\n",
        "      \n",
        "      input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "      attention_masks = []\n",
        "      # Create a mask of 1s for each token followed by 0s for padding\n",
        "      for seq in input_ids:\n",
        "        seq_mask = [float(i>0) for i in seq]\n",
        "        attention_masks.append(seq_mask)\n",
        "      self.inputs, self.masks, self.labels = torch.tensor(input_ids), torch.tensor(attention_masks), torch.tensor(self.labels)\n",
        "\n",
        "    def load(self, batch_size=32):\n",
        "      self.data = TensorDataset(self.inputs, self.masks, self.labels)\n",
        "      self.dataloader = DataLoader(self.data, shuffle=False, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbzxSKuwFiWv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_folder = \"datasets/train-articles\" # check that the path to the datasets folder is correct, \n",
        "dev_folder = \"datasets/dev-articles\"     # if not adjust these variables accordingly\n",
        "train_labels_file = \"datasets/train-task2-TC.labels\"\n",
        "dev_labels_file = \"datasets/dev-task-TC.labels\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEG_cDC1s3XB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dev=TransformerDataset(dev_folder, dev_labels_file)\n",
        "dev.tokenize()\n",
        "dev.encode()\n",
        "dev.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRqWncWsGdvk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=TransformerDataset(train_folder, train_labels_file)\n",
        "train.tokenize()\n",
        "train.encode()\n",
        "train.load()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L51eet5LGrbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_=TransformerModel()\n",
        "model_.freeze()\n",
        "model_.updater(lr=1e-4)\n",
        "model_.train(train,dev,epochs=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OJ-AXgAVahI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH=\"./BERT.pth\"\n",
        "torch.save(model_.transformer.state_dict(), PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFU-w0CLYgf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_dev_probs=model_.logits(dev)\n",
        "bert_train_probs=model_.logits(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHTkUTuylCRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ML_train=SLDataset(train_folder, train_labels_file)\n",
        "ML_dev=SLDataset(dev_folder, dev_labels_file)\n",
        "ML_train.clean()\n",
        "ML_dev.clean()\n",
        "features=FeatureExtraction(ML_train,ML_dev)\n",
        "features.get_features()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRun0HhOYVkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_lr=LogisticRegression(penalty='l2', class_weight='balanced', solver=\"liblinear\")\n",
        "model_lr.fit(features.train_features, ML_train.gold_labels)\n",
        "lr_train=model_lr.predict_proba(features.train_features)\n",
        "lr_dev=model_lr.predict_proba(features.dev_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAD0b_Q_YW3_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "_bert_train_probs=preprocessing.scale(bert_train_probs,axis=1)\n",
        "_bert_dev_probs=preprocessing.scale(bert_dev_probs,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_b0LJVNcYbfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta=LogisticRegression(penalty='l2', class_weight='balanced', solver=\"liblinear\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFM7tyAOYnbR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta.fit(np.concatenate([lr_train, _bert_train_probs],axis=1), ML_train.gold_labels)\n",
        "ans=meta.predict(np.concatenate([lr_dev, _bert_dev_probs],axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgMKZNwDYpCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print((ans==ML_dev.gold_labels).sum()/ML_dev.size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}