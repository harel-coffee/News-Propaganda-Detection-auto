{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text,shrink=False):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text=re.sub('[“\"”]',' \" ',text)\n",
    "    retain='[^abcdefghijklmnopqrstuvwxyz!#?\" ]'\n",
    "    text=re.sub('[()–-]',' ',text)\n",
    "    text=re.sub(retain,'',text)\n",
    "    text=text.replace('?',' ? ')\n",
    "    text=text.replace('#',' # ')\n",
    "    text=text.replace('!',' ! ')\n",
    "    text=text.split()\n",
    "    Lemmatizer=WordNetLemmatizer()\n",
    "    Stemmer=PorterStemmer()\n",
    "    if shrink:    \n",
    "        text=[Stemmer.stem(Lemmatizer.lemmatize(word, pos='v')) for word in text]\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory='datasets/train-task2-TC.labels'\n",
    "props_=open(directory).read().split('\\n')[:-1]\n",
    "sentences,Articles, labels=[],[],[]\n",
    "IDS=[]\n",
    "Spans=[]\n",
    "for prop in props_:\n",
    "    prop_=prop.split('\\t')\n",
    "    x,y=int(prop_[2]),int(prop_[3])\n",
    "    Spans.append((x,y))\n",
    "    sentences.append([open('datasets/train-articles/article{}.txt'.format(prop_[0])).read()[:x],open('datasets/train-articles/article{}.txt'.format(prop_[0])).read()[x:y],open('datasets/train-articles/article{}.txt'.format(prop_[0])).read()[y:]])\n",
    "    Articles.append(open('datasets/train-articles/article{}.txt'.format(prop_[0])).read())\n",
    "    labels.append(prop_[1])\n",
    "    IDS.append(prop_[0])\n",
    "    ids=int(prop_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind=2\n",
    "def gen(ind):\n",
    "    print(labels[ind])\n",
    "    pre=colored(sentences[ind][0], color='green')\n",
    "    sen=colored(sentences[ind][1], color='red')\n",
    "    post=colored(sentences[ind][-1], color='green')\n",
    "    print(pre+sen+post)\n",
    "def span_read(id,span):\n",
    "    return open('datasets/train-articles/article{}.txt'.format(id)).read()[span[0]:span[1]]\n",
    "def freq_dict(text,Stem=False):\n",
    "    freq={}\n",
    "    for word in clean(text,Stem).split():\n",
    "        if word in freq:\n",
    "            freq[word]=freq[word]+1\n",
    "        else:\n",
    "            freq[word]=1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Appeal_to_Authority',\n",
       " 'Appeal_to_fear-prejudice',\n",
       " 'Bandwagon,Reductio_ad_hitlerum',\n",
       " 'Black-and-White_Fallacy',\n",
       " 'Causal_Oversimplification',\n",
       " 'Doubt',\n",
       " 'Exaggeration,Minimisation',\n",
       " 'Flag-Waving',\n",
       " 'Loaded_Language',\n",
       " 'Name_Calling,Labeling',\n",
       " 'Repetition',\n",
       " 'Slogans',\n",
       " 'Thought-terminating_Cliches',\n",
       " 'Whataboutism,Straw_Men,Red_Herring'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'Id':IDS, 'Span':Spans, 'Label':labels})\n",
    "set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps=True\n",
    "if(not dumps):\n",
    "    Stitch=\"\"\n",
    "    for doc in os.listdir('datasets/train-articles'):\n",
    "        Stitch=Stitch+open('datasets/train-articles/'+doc).read()    \n",
    "    f = open(\"dumps/Text_stitch.txt\",\"w\")\n",
    "    f.write(Stitch)\n",
    "    f.close()\n",
    "    corp_freq=dict()\n",
    "    Doc_freqs=dict()\n",
    "    corp_freq=freq_dict(Stitch)\n",
    "    for id in IDS :\n",
    "        txt=open('datasets/train-articles/article{}.txt'.format(id)).read()\n",
    "        Doc_freqs[id]=freq_dict(txt)\n",
    "\n",
    "    corp_freq_json = json.dumps(corp_freq)\n",
    "    f = open(\"dumps/Corp_freq_raw.json\",\"w\")\n",
    "    f.write(corp_freq_json)\n",
    "    f.close()\n",
    "\n",
    "    Doc_freqs_json = json.dumps(Doc_freqs)\n",
    "    f = open(\"dumps/Doc_freqs_raw.json\",\"w\")\n",
    "    f.write(Doc_freqs_json)\n",
    "    f.close()\n",
    "\n",
    "    corp_freq_stem=dict()\n",
    "    Doc_freqs_stem=dict()\n",
    "    corp_freq_stem=freq_dict(Stitch,Stem=True)\n",
    "    for id in IDS :\n",
    "        txt=open('datasets/train-articles/article{}.txt'.format(id)).read()\n",
    "        Doc_freqs_stem[id]=freq_dict(txt,Stem=True)\n",
    "\n",
    "    corp_freq_json_stem = json.dumps(corp_freq_stem)\n",
    "    f = open(\"dumps/Corp_freq_stem.json\",\"w\")\n",
    "    f.write(corp_freq_json_stem)\n",
    "    f.close()\n",
    "\n",
    "    Doc_freqs_json_stem = json.dumps(Doc_freqs_stem)\n",
    "    f = open(\"dumps/Doc_freqs_stem.json\",\"w\")\n",
    "    f.write(Doc_freqs_json_stem)\n",
    "    f.close()\n",
    "\n",
    "try: \n",
    "    Stitch \n",
    "except: \n",
    "    Stitch=open(\"dumps/Text_stitch.txt\",'r').read()\n",
    "try: \n",
    "    corp_freq_raw \n",
    "except: \n",
    "    corp_freq_raw=json.loads(open(\"dumps/Corp_freq_raw.json\",'r').read())\n",
    "try: \n",
    "    Doc_freqs_raw \n",
    "except: \n",
    "    Doc_freqs_raw=json.loads(open(\"dumps/Doc_freqs_raw.json\",'r').read())\n",
    "try: \n",
    "    corp_freq_stem \n",
    "except: \n",
    "    corp_freq_stem=json.loads(open(\"dumps/Corp_freq_stem.json\",'r').read())\n",
    "try: \n",
    "    Doc_freqs_stem \n",
    "except: \n",
    "    Doc_freqs_stem=json.loads(open(\"dumps/Doc_freqs_stem.json\",'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(val):\n",
    "    t_df=df[df['Label']=='Doubt']\n",
    "    t_id,t_Span=t_df['Id'],t_df['Span']\n",
    "    padsent_d=[]\n",
    "    for ids,spans in zip(t_id,t_Span):\n",
    "        padsent_d.append(span_read(ids,(spans[0]-val,spans[1]+val)))\n",
    "\n",
    "    t_df=df[df['Label']!='Doubt']\n",
    "    t_id,t_Span=t_df['Id'],t_df['Span']\n",
    "    padsent=[]\n",
    "    for ids,spans in zip(t_id,t_Span):\n",
    "        padsent.append(span_read(ids,(spans[0]-val,spans[1]+val)))\n",
    "    return padsent_d,padsent\n",
    "padsent_d,padsent=pad(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_res(val,wts):\n",
    "    isin,notin=pad(val)\n",
    "    c,c1,t,t1=0,0,0,0\n",
    "    box=['?','how','when','where','why','while','what','may','cant']\n",
    "    for sent in isin:\n",
    "        t=t+1\n",
    "        #cond=False\n",
    "        for check in box:\n",
    "            if check in clean(sent).split():\n",
    "                c=c+wts[check]\n",
    "    for sent in notin:\n",
    "        t1=t1+1\n",
    "        #cond=False\n",
    "        for check in box:\n",
    "            if check in clean(sent).split():\n",
    "                c1=c1+wts[check]\n",
    "    print(c/t,c)\n",
    "    print(c1/t1,c1)\n",
    "    print((c*t1)/(c1*t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padsent_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a112ea776e6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;34m'cant'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m }\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-b2670291d504>\u001b[0m in \u001b[0;36mprint_res\u001b[0;34m(val, wts)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_res\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0misin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnotin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'how'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'when'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'where'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'why'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'while'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'what'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'may'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cant'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0misin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-f861fbb70c54>\u001b[0m in \u001b[0;36mpad\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspans\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_Span\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpadsent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspans\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadsent_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpadsent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'padsent_y' is not defined"
     ]
    }
   ],
   "source": [
    "wts={\n",
    "    '?' : 100,\n",
    "    'how' : 1,\n",
    "    'when' : 1,\n",
    "    'where' : 1,\n",
    "    'why' :1000,\n",
    "    'while' : 1,\n",
    "    'what' : 1,\n",
    "    'may' : 1,\n",
    "    'cant' : 1,\n",
    "}\n",
    "print_res(20,wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stitch=clean(Stitch)\n",
    "len(clean_stitch)/clean_stitch.count(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
