{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text=re.sub('[“\"”]',' \" ',text)\n",
    "    retain='[^abcdefghijklmnopqrstuvwxyz!#?\" ]'\n",
    "    text=re.sub('[()–-]',' ',text)\n",
    "    text=re.sub(retain,'',text)\n",
    "    text=text.replace('?',' ? ')\n",
    "    text=text.replace('#',' # ')\n",
    "    text=text.replace('!',' ! ')\n",
    "    text=text.split()\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder = \"datasets/train-articles\" # check that the path to the datasets folder is correct, \n",
    "dev_folder = \"datasets/dev-articles\"     # if not adjust these variables accordingly\n",
    "train_labels_file = \"datasets/train-task2-TC.labels\"\n",
    "dev_template_labels_file = \"datasets/dev-task-TC-template.out\"\n",
    "task_TC_output_file = \"baseline-output-TC.txt\"\n",
    "\n",
    "def read_articles_from_file_list(folder_name, file_pattern=\"*.txt\"):\n",
    "    \"\"\"\n",
    "    Read articles from files matching patterns <file_pattern> from  \n",
    "    the directory <folder_name>. \n",
    "    The content of the article is saved in the dictionary whose key\n",
    "    is the id of the article (extracted from the file name).\n",
    "    Each element of <sentence_list> is one line of the article.\n",
    "    \"\"\"\n",
    "    file_list = glob.glob(os.path.join(folder_name, file_pattern))\n",
    "    articles = {}\n",
    "    article_id_list, sentence_id_list, sentence_list = ([], [], [])\n",
    "    for filename in sorted(file_list):\n",
    "        article_id = os.path.basename(filename).split(\".\")[0][7:]\n",
    "        with codecs.open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            articles[article_id] = f.read()\n",
    "    return articles\n",
    "\n",
    "\n",
    "def read_predictions_from_file(filename):\n",
    "    \"\"\"\n",
    "    Reader for the gold file and the template output file. \n",
    "    Return values are four arrays with article ids, labels \n",
    "    (or ? in the case of a template file), begin of a fragment, \n",
    "    end of a fragment. \n",
    "    \"\"\"\n",
    "    articles_id, span_starts, span_ends, gold_labels = ([], [], [], [])\n",
    "    with open(filename, \"r\") as f:\n",
    "        for row in f.readlines():\n",
    "            article_id, gold_label, span_start, span_end = row.rstrip().split(\"\\t\")\n",
    "            articles_id.append(article_id)\n",
    "            gold_labels.append(gold_label)\n",
    "            span_starts.append(span_start)\n",
    "            span_ends.append(span_end)\n",
    "    return articles_id, span_starts, span_ends, gold_labels\n",
    "\n",
    "def compute_features_b(articles, span_starts, span_ends):\n",
    "    # only one feature, the length of the span\n",
    "    return np.array([ int(sp_ends)-int(sp_starts) for sp_starts, sp_ends in zip(span_starts, span_ends) ]).reshape(-1, 1)\n",
    "\n",
    "def clean(text):\n",
    "    text=text.lower()\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text=re.sub('[“\"”]',' \" ',text)\n",
    "    retain='[^abcdefghijklmnopqrstuvwxyz!#?\". ]'\n",
    "    text=re.sub('[()–-]',' ',text)\n",
    "    text=re.sub(retain,'',text)\n",
    "    text=re.sub('[.]',' . ',text)\n",
    "    text=text.replace('?',' ? ')\n",
    "    text=text.replace('#',' # ')\n",
    "    text=text.replace('!',' ! ')\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def read_span(id,span,dev=False):\n",
    "    if dev:\n",
    "        return dev_articles[id][span[0]:span[1]]\n",
    "    else:\n",
    "        return articles[id][span[0]:span[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6129 annotations from 357 articles\n",
      "Loaded 1063 annotations from 74 articles\n"
     ]
    }
   ],
   "source": [
    "# loading articles' content from *.txt files in the train folder\n",
    "articles = read_articles_from_file_list(train_folder)\n",
    "# loading gold labels, articles ids and sentence ids from files *.task-TC.labels in the train labels folder \n",
    "ref_articles_id, ref_span_starts, ref_span_ends, train_gold_labels = read_predictions_from_file(train_labels_file)\n",
    "print(\"Loaded %d annotations from %d articles\" % (len(ref_span_starts), len(set(ref_articles_id))))\n",
    "dev_articles = read_articles_from_file_list(dev_folder)\n",
    "dev_article_ids, dev_span_starts, dev_span_ends, dev_labels = read_predictions_from_file(dev_template_labels_file)\n",
    "print(\"Loaded %d annotations from %d articles\" % (len(dev_span_starts), len(set(dev_article_ids))))\n",
    "Train,Dev=pd.DataFrame(),pd.DataFrame()\n",
    "def find_sent(id,start,end,dev=False):\n",
    "    if dev:\n",
    "        dic=dev_articles\n",
    "    else:\n",
    "        dic=articles\n",
    "    x=dic[id].rfind('\\n',0,start)\n",
    "    if x == -1 :\n",
    "        x=0\n",
    "    y=dic[id].find('\\n',end)\n",
    "    x=max(x,dic[id].rfind('.',0,start))\n",
    "    y=min(y,dic[id].find('.',end))\n",
    "    return dic[id][x:y]\n",
    "Train['ID'],Dev['ID']=ref_articles_id,dev_article_ids\n",
    "Train['Start'],Train['End']=[int(i) for i in ref_span_starts], [int(i) for i in ref_span_ends]\n",
    "Dev['Start'],Dev['End']=[int(i) for i in dev_span_starts], [int(i) for i in dev_span_ends]\n",
    "Train['Span'],Dev['Span']=Train['End']-Train['Start'],Dev['End']-Dev['Start']\n",
    "Train['Sentence']=[articles[k][i:j] for i,j,k in zip(Train['Start'],Train['End'],Train['ID'])]\n",
    "Dev['Sentence']=[dev_articles[k][i:j] for i,j,k in zip(Dev['Start'],Dev['End'],Dev['ID'])]\n",
    "Train['Title']=Train['ID'].apply(lambda  x : articles[x].split('\\n')[0])\n",
    "Dev['Title']=Dev['ID'].apply(lambda  x : dev_articles[x].split('\\n')[0])\n",
    "Train['Target'],Dev['Target']=train_gold_labels,dev_labels\n",
    "Train['Paragraph']=[find_sent(id,start,end) for id,start,end in zip(Train['ID'],Train['Start'],Train['End'])]\n",
    "Dev['Paragraph']=[find_sent(id,start,end,dev=True) for id,start,end in zip(Dev['ID'],Dev['Start'],Dev['End'])]\n",
    "del Train['Start'],Train['End'],Dev['Start'],Dev['End']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    final=[]\n",
    "    for word in text.split():\n",
    "        if (('www' not in word) and ('http' not in word) and  ('.com' not in word)):\n",
    "            final.append(word)\n",
    "    final=re.sub('[^{} ]'.format(string.ascii_letters+string.digits+string.punctuation), '', ' '.join(final))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train['Paragraph'],Train['Title'],Train['Sentence']=Train['Paragraph'].apply(lambda x : clean(x)),Train['Title'].apply(lambda x : clean(x)),Train['Sentence'].apply(lambda x : clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Defeat Jihad\"'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final='Defeat Jihad\"'\n",
    "final=re.sub('[^{} ]'.format(string.ascii_letters+string.digits+string.punctuation), '', final)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop(['ID'],axis=1).to_csv('Train.csv',index=False)\n",
    "Dev.drop(['ID'],axis=1).to_csv('Dev.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
