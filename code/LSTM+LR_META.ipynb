{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LITmCq6UvVTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRAQ7BO2vg7Q",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.externals import joblib\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "# Keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, CuDNNLSTM, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "## Plotly\n",
        "import plotly.offline as py\n",
        "import plotly.graph_objs as go\n",
        "py.init_notebook_mode(connected=True)\n",
        "# Others\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.manifold import TSNE\n",
        "from keras.optimizers import SGD,Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.externals import joblib\n",
        "from io import StringIO\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_selection import chi2\n",
        "from IPython.display import display\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from xgboost import XGBClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hGx18revisN",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "train_folder = \"./drive/My Drive/datasets/train-articles\" # check that the path to the datasets folder is correct, \n",
        "dev_folder = \"./drive/My Drive/datasets/dev-articles\"     # if not adjust these variables accordingly\n",
        "train_labels_file = \"./drive/My Drive/datasets/train-task2-TC.labels\"\n",
        "dev_template_labels_file = \"./drive/My Drive/datasets/dev-task-TC-template.out\"\n",
        "task_TC_output_file = \"TFIDF_LR.txt\"\n",
        "\n",
        "def read_predictions_from_file(filename):\n",
        "    \"\"\"\n",
        "    Reader for the gold file and the template output file. \n",
        "    Return values are four arrays with article ids, labels \n",
        "    (or ? in the case of a template file), begin of a fragment, \n",
        "    end of a fragment. \n",
        "    \"\"\"\n",
        "    articles_id, span_starts, span_ends, gold_labels = ([], [], [], [])\n",
        "    with open(filename, \"r\") as f:\n",
        "        for row in f.readlines():\n",
        "            article_id, gold_label, span_start, span_end = row.rstrip().split(\"\\t\")\n",
        "            articles_id.append(article_id)\n",
        "            gold_labels.append(gold_label)\n",
        "            span_starts.append(span_start)\n",
        "            span_ends.append(span_end)\n",
        "    return articles_id, span_starts, span_ends, gold_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvsqkmWivkZO",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# loading gold labels, articles ids and sentence ids from files *.task-TC.labels in the train labels folder \n",
        "ref_articles_id, ref_span_starts, ref_span_ends, train_gold_labels = read_predictions_from_file(train_labels_file)\n",
        "print(\"Loaded %d annotations from %d articles\" % (len(ref_span_starts), len(set(ref_articles_id))))\n",
        "\n",
        "# reading data from the development set\n",
        "dev_article_ids, dev_span_starts, dev_span_ends, dev_labels = read_predictions_from_file(dev_template_labels_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br7A1ABQvq18",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "train=pd.read_csv('./drive/My Drive/datasets/train.csv').drop(['Unnamed: 0'],axis=1)\n",
        "dev=pd.read_csv('./drive/My Drive/datasets/dev.csv').drop(['Unnamed: 0'],axis=1)\n",
        "dev['Sentence']=dev['text']\n",
        "del dev['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MD8Rssy_MVI",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "import tensorflow as tf\n",
        "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())\n",
        "from keras import backend as K\n",
        "K.tensorflow_backend._get_available_gpus()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcMT8X1dwYe5",
        "colab_type": "code",
        "outputId": "45024a6e-6889-4d4e-f3e5-eef8919439b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        }
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-29 02:58:02--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2020-02-29 02:58:02--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2020-02-29 02:58:02--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  2.04MB/s    in 6m 27s  \n",
            "\n",
            "2020-02-29 03:04:29 (2.13 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX0_cvYPvsh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings_index = dict()\n",
        "f = open('./glove.6B.100d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNe--yMm4OfV",
        "colab_type": "code",
        "outputId": "b6c732b8-0f30-4e35-a09f-9474669c8168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''import nltk\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "train['Sentence']=train['Sentence'].apply(lambda x : ' '.join([word for word in x.split(' ') if (word not in stop_words and len(word)>1)]))\n",
        "dev['Sentence']=dev['Sentence'].apply(lambda x : ' '.join([word for word in x.split(' ') if (word not in stop_words and len(word)>1)]))'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import nltk\\nnltk.download(\\'stopwords\\')\\nstop_words = set(stopwords.words(\"english\"))\\ntrain[\\'Sentence\\']=train[\\'Sentence\\'].apply(lambda x : \\' \\'.join([word for word in x.split(\\' \\') if (word not in stop_words and len(word)>1)]))\\ndev[\\'Sentence\\']=dev[\\'Sentence\\'].apply(lambda x : \\' \\'.join([word for word in x.split(\\' \\') if (word not in stop_words and len(word)>1)]))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zVRaT3uv4hZ1",
        "colab_type": "code",
        "outputId": "7d8df32f-10ce-4edf-9a81-c20715d0c982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "(sorted(train['Sentence'].str.split().apply(len)))[-10:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[83, 87, 89, 92, 98, 101, 109, 109, 147, 157]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbZMb3YT2Iyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocabulary_size = 8300\n",
        "tokenizer = Tokenizer(num_words= vocabulary_size)\n",
        "tokenizer.fit_on_texts(train['Sentence'].append(dev['Sentence']))\n",
        "sequences = tokenizer.texts_to_sequences(train['Sentence'])\n",
        "data = pad_sequences(sequences, maxlen=90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-FS80EAwKp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OOV=[]\n",
        "embedding_matrix = np.zeros((vocabulary_size, 100))\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index > vocabulary_size - 1 :\n",
        "        break\n",
        "    else:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[index] = embedding_vector\n",
        "        else:\n",
        "            OOV.append(word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1AylPhm2seF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ttfidf = TfidfVectorizer(sublinear_tf=True, min_df=5,ngram_range=(1,6),stop_words='english',analyzer='char',max_features=2750)\n",
        "ttfidf_w = TfidfVectorizer(sublinear_tf=True, min_df=1,ngram_range=(1,1),stop_words='english',analyzer='word',max_features=3000)\n",
        "len_v= train['Span'].values.reshape(-1,1)\n",
        "DF=pd.concat([train,dev])\n",
        "features = ttfidf.fit_transform(DF.Sentence).toarray().astype('float64')\n",
        "features_w = ttfidf_w.fit_transform(DF.Sentence).toarray().astype('float64')\n",
        "features = np.append(features,DF['Span'].values.reshape(-1,1),axis=1)\n",
        "features= np.append(features,features_w,axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qubf6cYLWAvl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train_X, Train_Y = features[:len(train)], DF[:len(train)]['Target']\n",
        "Test=features[len(train):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dd7GMsm5T4N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing \n",
        "from keras.utils import np_utils\n",
        "# label_encoder object knows how to understand word labels. \n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "y=label_encoder.fit_transform(train.Target)\n",
        "y=np_utils.to_categorical(y)\n",
        "test_sequences = tokenizer.texts_to_sequences(dev['Sentence'])\n",
        "test_data = pad_sequences(test_sequences, maxlen=90) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZGRSeeeFTZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pred_file(pred,x=True):\n",
        "  if x:\n",
        "    pred=label_encoder.inverse_transform(pred.argmax(axis=1)).tolist()\n",
        "  task_TC_output_file='LSTM.txt'\n",
        "  ##### writing predictions to file\n",
        "  with open(task_TC_output_file, \"w\") as fout:\n",
        "      for article_id, prediction, span_start, span_end in zip(dev_article_ids, pred, dev_span_starts, dev_span_ends):\n",
        "          fout.write(\"%s\\t%s\\t%s\\t%s\\n\" % (article_id, prediction, span_start, span_end))\n",
        "  print(\"Predictions written to file \" + task_TC_output_file)\n",
        "  from google.colab import files\n",
        "  files.download('./LSTM.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJSZQxWaF8fw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model(history,rng=[0,-1]):\n",
        "  plt.plot(history.history['acc'][rng[0]:rng[1]])\n",
        "  plt.plot(history.history['val_acc'][rng[0]:rng[1]])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'][rng[0]:rng[1]])\n",
        "  plt.plot(history.history['val_loss'][rng[0]:rng[1]])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Valid'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un64tz4jYhCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Embedding, LSTM, Dense, concatenate\n",
        "from keras.models import Model\n",
        "from keras.models import model_from_json\n",
        "SAVED=True\n",
        "#Functional Model\n",
        "MAX_SEQUENCE_LENGTH=90\n",
        "EMBEDDING_DIM=100\n",
        "\n",
        "embedding_layer = Embedding(vocabulary_size,EMBEDDING_DIM,weights=[embedding_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=False)\n",
        "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
        "embedded_sequences = embedding_layer(sequence_input)\n",
        "embedded_sequences = Dropout(0.4)(embedded_sequences)\n",
        "LSTM = CuDNNLSTM(100, return_sequences=True)(embedded_sequences)\n",
        "LSTM = Dropout(0.4)(LSTM)\n",
        "LSTM = CuDNNLSTM(100)(LSTM)\n",
        "Output= Dense(14, activation='sigmoid')(LSTM)\n",
        "opt = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=True, clipnorm=1)\n",
        "model = Model(sequence_input,Output)\n",
        "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['acc'])\n",
        "if SAVED:\n",
        "  model.load_weights(\"./drive/My Drive/datasets/model.h5\")\n",
        "else:\n",
        "  hist=model.fit(data, y, validation_split=0.2, epochs=100, batch_size=64, verbose=1)\n",
        "  plot_model(hist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7fa8dUa5EC5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred=model.predict(data)\n",
        "assemble=lambda features : np.concatenate(features,axis=1)\n",
        "model_lr=LogisticRegression(penalty='l2', class_weight='balanced', solver=\"liblinear\", max_iter=500)\n",
        "model_lr.fit(Train_X, Train_Y)\n",
        "pred_lr=model_lr.predict_proba(Train_X)\n",
        "t_x=assemble([pred_lr,pred])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "leLQiOBO4rf1",
        "colab": {}
      },
      "source": [
        "if not SAVED:\n",
        "  # serialize model to JSON\n",
        "  model_json = model.to_json()\n",
        "  with open(\"model.json\", \"w\") as json_file:\n",
        "      json_file.write(model_json)\n",
        "  # serialize weights to HDF5\n",
        "  model.save_weights(\"model.h5\")\n",
        "  print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "24J5j6DuLGO4",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_XN0h6C7ktd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,colsample_bynode=1, colsample_bytree=0.8, gamma=4,learning_rate=0.02, max_delta_step=0, max_depth=4,min_child_weight=5, n_estimators=600,njobs=4, objective='multi:softmax', num_classes=14, scale_pos_weight=1, seed=1234, subsample=0.7, verbosity=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "117FUph27kwg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fb806bc-aa74-4260-d613-5be283fad78b"
      },
      "source": [
        "print('\\n All results:')\n",
        "print(random_search.cv_results_)\n",
        "print('\\n Best estimator:')\n",
        "print(random_search.best_estimator_)\n",
        "print('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\n",
        "print(random_search.best_score_ * 2 - 1)\n",
        "print('\\n Best hyperparameters:')\n",
        "print(random_search.best_params_)\n",
        "results = pd.DataFrame(random_search.cv_results_)\n",
        "results.to_csv('xgb-random-grid-search-results-01.csv', index=False)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " All results:\n",
            "{'mean_fit_time': array([197.52205485, 145.19374537, 145.14607829, 180.79496837,\n",
            "       181.86118245, 200.24097097, 195.77097422, 204.44364834,\n",
            "       133.00103068, 187.23906386, 180.53325224, 195.08657426]), 'std_fit_time': array([ 3.00392534,  8.60132374,  0.25341506,  1.12894079,  0.66717236,\n",
            "        0.93027562,  0.2181088 ,  0.95091138,  0.21066781,  0.66887603,\n",
            "        0.5650523 , 24.23537994]), 'mean_score_time': array([2.51676869, 2.35804206, 2.4240573 , 2.56846321, 2.30962676,\n",
            "       2.42475122, 2.63364005, 2.3195281 , 2.19422662, 2.35056794,\n",
            "       2.40110558, 1.13423133]), 'std_score_time': array([0.03030434, 0.20628705, 0.06444117, 0.08841918, 0.02714269,\n",
            "       0.01374618, 0.03781214, 0.02613819, 0.04090508, 0.0542496 ,\n",
            "       0.1741034 , 0.47413426]), 'param_subsample': masked_array(data=[0.8, 0.8, 0.8, 0.7, 0.7, 0.8, 0.7, 0.7, 0.7, 0.7, 0.8,\n",
            "                   0.7],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_min_child_weight': masked_array(data=[5, 5, 7, 7, 7, 5, 5, 7, 7, 4, 7, 5],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_max_depth': masked_array(data=[4, 2, 2, 3, 3, 3, 3, 4, 2, 3, 3, 4],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_gamma': masked_array(data=[5, 4, 4, 4, 5, 5, 4, 7, 7, 7, 5, 4],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'param_colsample_bytree': masked_array(data=[0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.7, 0.7, 0.7, 0.7,\n",
            "                   0.8],\n",
            "             mask=[False, False, False, False, False, False, False, False,\n",
            "                   False, False, False, False],\n",
            "       fill_value='?',\n",
            "            dtype=object), 'params': [{'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 4, 'gamma': 5, 'colsample_bytree': 0.7}, {'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 2, 'gamma': 4, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 7, 'max_depth': 2, 'gamma': 4, 'colsample_bytree': 0.8}, {'subsample': 0.7, 'min_child_weight': 7, 'max_depth': 3, 'gamma': 4, 'colsample_bytree': 0.8}, {'subsample': 0.7, 'min_child_weight': 7, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 0.8}, {'subsample': 0.7, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 4, 'colsample_bytree': 0.8}, {'subsample': 0.7, 'min_child_weight': 7, 'max_depth': 4, 'gamma': 7, 'colsample_bytree': 0.7}, {'subsample': 0.7, 'min_child_weight': 7, 'max_depth': 2, 'gamma': 7, 'colsample_bytree': 0.7}, {'subsample': 0.7, 'min_child_weight': 4, 'max_depth': 3, 'gamma': 7, 'colsample_bytree': 0.7}, {'subsample': 0.8, 'min_child_weight': 7, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 0.7}, {'subsample': 0.7, 'min_child_weight': 5, 'max_depth': 4, 'gamma': 4, 'colsample_bytree': 0.8}], 'split0_test_score': array([0.83953033, 0.84018265, 0.83953033, 0.84018265, 0.83953033,\n",
            "       0.83757339, 0.83887802, 0.83953033, 0.84344423, 0.8382257 ,\n",
            "       0.83692107, 0.84279191]), 'split1_test_score': array([0.82441253, 0.82375979, 0.82180157, 0.82049608, 0.82114883,\n",
            "       0.82114883, 0.82114883, 0.82114883, 0.8191906 , 0.82180157,\n",
            "       0.82114883, 0.82571802]), 'split2_test_score': array([0.82832898, 0.82637076, 0.82767624, 0.83159269, 0.82898172,\n",
            "       0.82898172, 0.83224543, 0.82506527, 0.82245431, 0.82506527,\n",
            "       0.82832898, 0.83289817]), 'split3_test_score': array([0.81005222, 0.81201044, 0.81201044, 0.80939948, 0.80678851,\n",
            "       0.80613577, 0.81396867, 0.81005222, 0.81201044, 0.80809399,\n",
            "       0.80809399, 0.81527415]), 'mean_test_score': array([0.82558102, 0.82558091, 0.82525465, 0.82541772, 0.82411235,\n",
            "       0.82345993, 0.82656024, 0.82394916, 0.82427489, 0.82329663,\n",
            "       0.82362322, 0.82917056]), 'std_test_score': array([0.01054329, 0.01001634, 0.00996233, 0.01158598, 0.01194072,\n",
            "       0.01156663, 0.0096421 , 0.01054734, 0.01169449, 0.01071635,\n",
            "       0.01056239, 0.01005568]), 'rank_test_score': array([ 3,  4,  6,  5,  8, 11,  2,  9,  7, 12, 10,  1], dtype=int32)}\n",
            "\n",
            " Best estimator:\n",
            "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
            "              colsample_bynode=1, colsample_bytree=0.8, gamma=4,\n",
            "              learning_rate=0.02, max_delta_step=0, max_depth=4,\n",
            "              min_child_weight=5, missing=None, n_estimators=600, n_jobs=1,\n",
            "              njobs=4, nthread=None, objective='multi:softprob', random_state=0,\n",
            "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
            "              silent=True, subsample=0.7, verbosity=1)\n",
            "\n",
            " Best normalized gini score for 4-fold search with 12 parameter combinations:\n",
            "0.6583411253553249\n",
            "\n",
            " Best hyperparameters:\n",
            "{'subsample': 0.7, 'min_child_weight': 5, 'max_depth': 4, 'gamma': 4, 'colsample_bytree': 0.8}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osjI48CD7krI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_lr=model_lr.predict_proba(Test)\n",
        "preds=model.predict(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_IiQO5ScxMi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "meta.fit(t_x,Train_Y)\n",
        "meta_pred=meta.predict(assemble([pred_lr,preds]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlmVSu-RrhN2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "45daec6f-dba4-4f0b-ecbd-62fdc84e710a"
      },
      "source": [
        "pred_file(meta_pred,x=False)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions written to file LSTM.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6wxhWNNeJgf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}